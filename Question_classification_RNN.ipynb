{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Resources/LabelledData.csv', header = None, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "training = []\n",
    "line_counter = 0\n",
    "with open(\"Resources/LabelledData.csv\",'r') as f:\n",
    "    tsv = csv.reader(f, dialect='excel', delimiter='\\t')\n",
    "    for line in tsv:\n",
    "        if line_counter < 900:\n",
    "            training.append(line[0])\n",
    "        else:\n",
    "            break\n",
    "            line_counter = line_counter+1\n",
    "    # training_data = list(training)\n",
    "    training_data = [s.strip() for s in training]\n",
    "    # Split by words\n",
    "    x_text = training_data\n",
    "    x_text = [clean_str(sent) for sent in x_text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('check.txt','wt')\n",
    "label =[]\n",
    "labels=[]\n",
    "line_counter = 0\n",
    "with open(\"Resources/LabelledData.csv\",'r') as f:\n",
    "    tsv = csv.reader(f, dialect='excel', delimiter='\\t')\n",
    "    for line in tsv:\n",
    "        if line_counter<900:\n",
    "            label.append(line[1])\n",
    "        else:\n",
    "            break\n",
    "            line_counter =  line_counter +1\n",
    "    labels =label\n",
    "    tempList = []\n",
    "    labelList = []\n",
    "    for a in label:\n",
    "        t.write(a)\n",
    "        t.write('--')\n",
    "        if not a in tempList:\n",
    "            tempList.append(a)\n",
    "            labelList.append(tempList.index(a))\n",
    "            t.write(str(tempList.index(a)))\n",
    "            t.write('\\n')\n",
    "        else:\n",
    "            labelList.append(tempList.index(a))\n",
    "    n_labels = len(labelList)\n",
    "    target = []\n",
    "    for x in range(0,n_labels):\n",
    "        listFori = []\n",
    "        i=0\n",
    "        while i < len(tempList):\n",
    "            if i == labelList[x]:\n",
    "                listFori.append(1)\n",
    "            else:\n",
    "                listFori.append(0)\n",
    "            i = i+1\n",
    "        target.append(listFori)\n",
    "    y = np.concatenate([target],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 30\n",
    "epochs =30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR='./GLOVE.6B/'\n",
    "embeddings_index = {}\n",
    "FILE=os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "f = open(FILE,encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1483\n",
      "unique words : 3674\n",
      "Shape of data tensor: (1483, 1000)\n",
      "Shape of label tensor: (1483, 5)\n",
      "1483\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "print(tokenizer.fit_on_texts(x_text))\n",
    "sequences =  tokenizer.texts_to_sequences(x_text)\n",
    "print(len(sequences))\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(y))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "y = y[indices]\n",
    "nb_validation_samples = int(0.10 * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = y[:-nb_validation_samples]\n",
    "x_test = data[-nb_validation_samples:]\n",
    "y_test = y[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                               input_length=MAX_SEQUENCE_LENGTH,\n",
    "                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         367500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 414,225\n",
      "Trainable params: 414,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,output_dim=100,weights=None,trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add( LSTM(64, return_sequences=False, dropout=0.1,recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chindukuri\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1335/1335 [==============================] - 108s 81ms/step - loss: 1.4463 - accuracy: 0.4060\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chindukuri\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335/1335 [==============================] - 107s 80ms/step - loss: 1.1097 - accuracy: 0.5543\n",
      "Epoch 3/15\n",
      "1335/1335 [==============================] - 107s 81ms/step - loss: 0.5226 - accuracy: 0.8307\n",
      "Epoch 4/15\n",
      "1335/1335 [==============================] - 111s 83ms/step - loss: 0.3374 - accuracy: 0.8509\n",
      "Epoch 5/15\n",
      "1335/1335 [==============================] - 115s 86ms/step - loss: 0.2518 - accuracy: 0.8966\n",
      "Epoch 6/15\n",
      "1335/1335 [==============================] - 117s 88ms/step - loss: 0.1698 - accuracy: 0.9468\n",
      "Epoch 7/15\n",
      "1335/1335 [==============================] - 124s 93ms/step - loss: 0.1010 - accuracy: 0.9760\n",
      "Epoch 8/15\n",
      "1335/1335 [==============================] - 130s 98ms/step - loss: 0.0370 - accuracy: 0.9948\n",
      "Epoch 9/15\n",
      "1335/1335 [==============================] - 138s 103ms/step - loss: 0.0234 - accuracy: 0.9970\n",
      "Epoch 10/15\n",
      "1335/1335 [==============================] - 143s 107ms/step - loss: 0.0187 - accuracy: 0.9955\n",
      "Epoch 11/15\n",
      "1335/1335 [==============================] - 147s 110ms/step - loss: 0.0375 - accuracy: 0.9925\n",
      "Epoch 12/15\n",
      "1335/1335 [==============================] - 155s 116ms/step - loss: 0.0121 - accuracy: 0.9985\n",
      "Epoch 13/15\n",
      "1335/1335 [==============================] - 164s 123ms/step - loss: 0.0127 - accuracy: 0.9993\n",
      "Epoch 14/15\n",
      "1335/1335 [==============================] - 167s 125ms/step - loss: 0.0064 - accuracy: 0.9993\n",
      "Epoch 15/15\n",
      "1335/1335 [==============================] - 179s 134ms/step - loss: 0.0044 - accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x288224c56a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, batch_size=20, epochs=15, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3508598127880612\n",
      "Test accuracy: 0.9527027010917664\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)  \n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "148/148 [==============================] - 4s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test, batch_size=1024,   verbose=1)\n",
    "labels = ['unknown', 'what', 'when', 'who', 'affirmation']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['who',\n",
       " 'affirmation',\n",
       " 'who',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'who',\n",
       " 'affirmation',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'when',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'what',\n",
       " 'when',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'when',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'who',\n",
       " 'when',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'when',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'affirmation',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'who',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'when',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'affirmation',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'who',\n",
       " 'unknown']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " q=[\n",
    "'what fowl grabs the spotlight after the chinese year of the monkey',\n",
    "'what is the full form of .com',\n",
    "'what contemptible scoundrel stole the cork from my lunch',\n",
    "'what team did baseball s st. louis browns become',\n",
    "'what is the oldest profession',\n",
    "'what are liver enzymes',\n",
    "'name the scar-faced bounty hunter of the old west', \n",
    "'when was ozzy osbourne born',\n",
    "'why do heavier objects travel downhill faster',\n",
    "'who was the pride of the yankees',\n",
    "'who killed gandhi',\n",
    "'what is considered the costliest disaster the insurance industry has ever faced',\n",
    "'what sprawling u.s. state boasts the most airports',\n",
    "'what did the only repealed amendment to the u.s. constitution deal with',\n",
    "'how many jews were executed in concentration camps during wwii',\n",
    "'what is nine inch nails',\n",
    "'what is an annotated bibliography',\n",
    "'what is the date of boxing day',\n",
    "'what articles of clothing are tokens in monopoly',\n",
    "'name 11 famous martyrs',\n",
    "'what is the olympic motto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "21\n",
      "unique words : 106\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "print(tokenizer.fit_on_texts(q))\n",
    "sequences =  tokenizer.texts_to_sequences(q)\n",
    "print(len(sequences))\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "21/21 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(data, batch_size=1024,   verbose=1)\n",
    "labels = ['unknown', 'what', 'when', 'who', 'affirmation']\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affirmation',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'what',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'when',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'unknown',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'who',\n",
       " 'what',\n",
       " 'what',\n",
       " 'unknown',\n",
       " 'what']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
